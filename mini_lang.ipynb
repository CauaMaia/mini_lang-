{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d6960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biblioteca para trabalhar com expressões regulares na análise léxica\n",
    "import re\n",
    "\n",
    "# Biblioteca para criar classes de dados de forma mais limpa\n",
    "from dataclasses import dataclass, field \n",
    "\n",
    "# Tipos para anotação de tipos em Python, melhorando a legibilidade do código\n",
    "from typing import List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccb63cc",
   "metadata": {},
   "source": [
    "**Tokens e Lexer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d5d371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define os padrões de todos os tokens que nosso analisador léxico pode reconhecer\n",
    "# Cada tupla contém o nome do token e sua expressão regular correspondente\n",
    "TOKEN_SPEC = [\n",
    "    (\"COMMENT\",      r\"//[^\\n]*\"),      # Comentários de linha que começam com //\n",
    "    (\"WS\",           r\"\\s+\"),           # Espaços em branco (espaços, tabs, quebras de linha)\n",
    "    (\"ANDAND\",       r\"&&\"),            # Operador lógico AND\n",
    "    (\"OROR\",         r\"\\|\\|\"),          # Operador lógico OR\n",
    "    (\"EQEQ\",         r\"==\"),            # Operador de igualdade\n",
    "    (\"NEQ\",          r\"!=\"),            # Operador de diferença\n",
    "    (\"LE\",           r\"<=\"),            # Menor ou igual\n",
    "    (\"GE\",           r\">=\"),            # Maior ou igual\n",
    "    (\"ASSIGN\",       r\"=\"),             # Operador de atribuição\n",
    "    (\"LT\",           r\"<\"),             # Menor que\n",
    "    (\"GT\",           r\">\"),             # Maior que\n",
    "    (\"PLUS\",         r\"\\+\"),            # Soma\n",
    "    (\"MINUS\",        r\"-\"),             # Subtração\n",
    "    (\"STAR\",         r\"\\*\"),            # Multiplicação\n",
    "    (\"SLASH\",        r\"/\"),             # Divisão\n",
    "    (\"PERCENT\",      r\"%\"),             # Módulo (resto da divisão)\n",
    "    (\"BANG\",         r\"!\"),             # Negação lógica\n",
    "    (\"LPAREN\",       r\"\\(\"),            # Parêntese esquerdo\n",
    "    (\"RPAREN\",       r\"\\)\"),            # Parêntese direito\n",
    "    (\"LBRACE\",       r\"\\{\"),            # Chave esquerda\n",
    "    (\"RBRACE\",       r\"\\}\"),            # Chave direita\n",
    "    (\"SEMI\",         r\";\"),             # Ponto e vírgula\n",
    "    (\"COMMA\",        r\",\"),             # Vírgula\n",
    "    (\"NUMBER\",       r\"\\d+\"),           # Números inteiros\n",
    "    (\"IDENT\",        r\"[A-Za-z_][A-Za-z0-9_]*\"), # Identificadores (nomes de variáveis, funções, etc.)\n",
    "]\n",
    "\n",
    "# Palavras reservadas da linguagem que têm significado especial\n",
    "# Quando encontramos um identificador que está neste dicionário, o tratamos como palavra-chave\n",
    "KEYWORDS = {\n",
    "    \"int\": \"INT\",       # Tipo inteiro\n",
    "    \"bool\": \"BOOL\",     # Tipo booleano\n",
    "    \"true\": \"TRUE\",     # Valor verdadeiro\n",
    "    \"false\": \"FALSE\",   # Valor falso\n",
    "    \"if\": \"IF\",         # Condicional se\n",
    "    \"else\": \"ELSE\",     # Condicional senão\n",
    "    \"while\": \"WHILE\",   # Loop enquanto\n",
    "}\n",
    "\n",
    "# Compila todas as expressões regulares dos tokens em uma única expressão grande\n",
    "# Isso permite que possamos verificar todos os padrões de uma vez só\n",
    "MASTER_RE = re.compile(\"|\".join(f\"(?P<{name}>{pattern})\" for name, pattern in TOKEN_SPEC))\n",
    "\n",
    "@dataclass\n",
    "class Token:\n",
    "    \"\"\"Representa um token encontrado no código fonte\"\"\"\n",
    "    type: str      # Tipo do token (NUMBER, IDENT, IF, etc.)\n",
    "    lexeme: str    # O texto original que formou este token\n",
    "    line: int      # Linha onde o token foi encontrado\n",
    "    col: int       # Coluna onde o token começou\n",
    "\n",
    "class Lexer:\n",
    "    \"\"\"\n",
    "    Analisador léxico que converte texto em uma sequência de tokens.\n",
    "    Produz:\n",
    "      - lista de tokens encontrados\n",
    "      - 'tabela de lexemas e tokens' (é basicamente a mesma lista com metadados)\n",
    "      - cadeia de tokens (string compacta dos tipos)\n",
    "    Ignora espaços em branco e comentários. Reporta erro léxico com posição exata.\n",
    "    \"\"\"\n",
    "    def __init__(self, source: str):\n",
    "        self.source = source              # Código fonte a ser analisado\n",
    "        self.tokens: List[Token] = []     # Lista onde armazenamos os tokens encontrados\n",
    "\n",
    "    def tokenize(self) -> List[Token]:\n",
    "        \"\"\"Percorre o código fonte e extrai todos os tokens válidos\"\"\"\n",
    "        line = 1                         # Linha atual (começa em 1)\n",
    "        line_start_idx = 0               # Índice onde a linha atual começou\n",
    "        i = 0                            # Posição atual no código fonte\n",
    "        s = self.source                  # Referência local para o código fonte\n",
    "        \n",
    "        while i < len(s):                # Enquanto não chegamos ao fim do código\n",
    "            m = MASTER_RE.match(s, i)    # Tenta fazer match de qualquer token na posição atual\n",
    "            \n",
    "            if not m:\n",
    "                # Se não conseguiu reconhecer nenhum token, temos um erro léxico\n",
    "                ch = s[i]                \n",
    "                col = i - line_start_idx + 1  \n",
    "                raise SyntaxError(f\"[Lex] Caractere inesperado '{ch}' em {line}:{col}\") \n",
    "            \n",
    "            # Extraímos informações sobre o token encontrado\n",
    "            kind = m.lastgroup           # Tipo do token (qual grupo da regex fez match)\n",
    "            lexeme = m.group(kind)       # Texto que formou o token\n",
    "            start = i                    # Posição onde o token começou\n",
    "            end = m.end()                # Posição onde o token terminou\n",
    "            newlines = lexeme.count(\"\\n\") # Quantas quebras de linha há no token\n",
    "            \n",
    "            # Se for espaço em branco ou comentário, ignoramos mas atualizamos a posição\n",
    "            if kind == \"WS\" or kind == \"COMMENT\":\n",
    "                i = end                  \n",
    "                if newlines:\n",
    "                    line += newlines     # Atualiza número da linha\n",
    "                    last_nl = s.rfind(\"\\n\", start, end) \n",
    "                    line_start_idx = last_nl + 1        # Atualiza onde a linha atual começou\n",
    "                continue                 \n",
    "\n",
    "            # Verifica se o identificador é uma palavra-chave\n",
    "            if kind == \"IDENT\" and lexeme in KEYWORDS:\n",
    "                tok_type = KEYWORDS[lexeme]  # Usa o tipo da palavra-chave\n",
    "            else:\n",
    "                tok_type = kind              # Usa o tipo normal do token\n",
    "\n",
    "            # Calcula a coluna e adiciona o token à lista\n",
    "            col = start - line_start_idx + 1 \n",
    "            self.tokens.append(Token(tok_type, lexeme, line, col)) \n",
    "            \n",
    "            # Avança para a próxima posição\n",
    "            i = end                         \n",
    "            if newlines:\n",
    "                line += newlines            \n",
    "                last_nl = s.rfind(\"\\n\", start, end) \n",
    "                line_start_idx = last_nl + 1        \n",
    "\n",
    "        # Adiciona token especial EOF (End Of File) para marcar o fim\n",
    "        self.tokens.append(Token(\"EOF\", \"\", line, (i - line_start_idx + 1))) \n",
    "        return self.tokens                \n",
    "\n",
    "    def token_chain(self) -> str:\n",
    "        \"\"\"Retorna uma string com os tipos dos tokens separados por espaço (útil para debug)\"\"\"\n",
    "        return \" \".join(tok.type for tok in self.tokens if tok.type != \"EOF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de87aae",
   "metadata": {},
   "source": [
    "**PARSER (Recursive Descent)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f02838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exceção personalizada para erros de análise sintática\n",
    "class ParseError(Exception):  \n",
    "    pass\n",
    "\n",
    "class Parser:\n",
    "    \"\"\"\n",
    "    Analisador sintático LL(1) que verifica se a sequência de tokens\n",
    "    segue as regras gramaticais da nossa mini linguagem.\n",
    "    \n",
    "    Usa a técnica de descida recursiva, onde cada regra da gramática\n",
    "    vira um método que chama outros métodos conforme necessário.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokens: List[Token]):  \n",
    "        self.tokens = tokens  # Lista de tokens para analisar\n",
    "        self.pos = 0          # Posição atual na lista de tokens\n",
    "\n",
    "    def peek(self) -> Token:  \n",
    "        \"\"\"Olha o token atual sem consumi-lo\"\"\"\n",
    "        return self.tokens[self.pos]\n",
    "\n",
    "    def advance(self) -> Token:  \n",
    "        \"\"\"Consome o token atual e avança para o próximo\"\"\"\n",
    "        tok = self.tokens[self.pos]\n",
    "        self.pos += 1\n",
    "        return tok\n",
    "\n",
    "    def match(self, *types) -> bool:  \n",
    "        \"\"\"Verifica se o token atual é de um dos tipos esperados e o consome se for\"\"\"\n",
    "        if self.peek().type in types:\n",
    "            self.advance()\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def expect(self, ttype: str, msg: str):  \n",
    "        \"\"\"Exige que o token atual seja de um tipo específico, senão gera erro\"\"\"\n",
    "        if self.peek().type == ttype:\n",
    "            return self.advance()\n",
    "        t = self.peek()\n",
    "        raise ParseError(f\"{msg} (encontrado {t.type} '{t.lexeme}' em {t.line}:{t.col})\")\n",
    "\n",
    "    def expect_ident_lexeme(self, name: str, msg: str):\n",
    "        \"\"\"Exige um identificador com um texto específico (exemplo: 'main')\"\"\"\n",
    "        t = self.peek()\n",
    "        if t.type == \"IDENT\" and t.lexeme == name:\n",
    "            return self.advance()\n",
    "        raise ParseError(f\"{msg} (encontrado {t.type} '{t.lexeme}' em {t.line}:{t.col})\")\n",
    "\n",
    "    def look_type(self, k: int) -> str:\n",
    "        \"\"\"Olha o tipo do token que está k posições à frente (1 = próximo token)\"\"\"\n",
    "        idx = self.pos + k\n",
    "        if idx < len(self.tokens):\n",
    "            return self.tokens[idx].type\n",
    "        return \"EOF\"\n",
    "\n",
    "    def parse(self):  \n",
    "        \"\"\"Método principal que inicia a análise sintática\"\"\"\n",
    "        self.program()  # Analisa o programa completo\n",
    "        self.expect(\"EOF\", \"Esperado fim do arquivo\")  # Garante que chegamos ao final\n",
    "        return True\n",
    "\n",
    "    def program(self):  \n",
    "        \"\"\"Programa deve começar com 'main() { ... }'\"\"\"\n",
    "        self.expect_ident_lexeme(\"main\", \"Esperado identificador 'main'\")\n",
    "        self.expect(\"LPAREN\", \"Esperado '(' após 'main'\")\n",
    "        self.expect(\"RPAREN\", \"Esperado ')' após '('\")\n",
    "        self.block()  # Analisa o bloco principal\n",
    "\n",
    "    def block(self):  \n",
    "        \"\"\"Bloco: '{' declarações* comandos* '}'\"\"\"\n",
    "        self.expect(\"LBRACE\", \"Esperado '{' para iniciar bloco\")\n",
    "        \n",
    "        # Primeiro vêm todas as declarações de variáveis\n",
    "        while self.peek().type in (\"INT\", \"BOOL\"):  \n",
    "            self.decl()\n",
    "        \n",
    "        # Depois vêm os comandos\n",
    "        while self.peek().type in (\"IF\", \"WHILE\", \"IDENT\", \"LBRACE\"):\n",
    "            self.stmt()\n",
    "            \n",
    "        self.expect(\"RBRACE\", \"Esperado '}' para fechar bloco\")\n",
    "\n",
    "    def decl(self):  \n",
    "        \"\"\"Declaração de variável: tipo identificador [= expressão] ;\"\"\"\n",
    "        if not self.match(\"INT\", \"BOOL\"):\n",
    "            t = self.peek()\n",
    "            raise ParseError(f\"Esperado tipo (int/bool), encontrado {t.type} em {t.line}:{t.col}\")\n",
    "        \n",
    "        self.expect(\"IDENT\", \"Esperado identificador na declaração\")\n",
    "        \n",
    "        # Inicialização é opcional\n",
    "        if self.match(\"ASSIGN\"):\n",
    "            self.expr()  # Analisa a expressão de inicialização\n",
    "            \n",
    "        self.expect(\"SEMI\", \"Esperado ';' ao final da declaração\")\n",
    "\n",
    "    def stmt(self):  \n",
    "        \"\"\"Analisa diferentes tipos de comandos baseado no primeiro token\"\"\"\n",
    "        t = self.peek().type\n",
    "        \n",
    "        if t == \"IF\":\n",
    "            self.if_stmt()\n",
    "        elif t == \"WHILE\":\n",
    "            self.while_stmt()\n",
    "        elif t == \"IDENT\":\n",
    "            # Precisa olhar à frente para decidir que tipo de comando é\n",
    "            if (self.look_type(1) == \"LPAREN\"\n",
    "                and self.look_type(2) == \"RPAREN\"\n",
    "                and self.look_type(3) == \"LBRACE\"):\n",
    "                # É uma declaração de função: nome() { ... }\n",
    "                self.func_decl()\n",
    "            elif self.look_type(1) == \"ASSIGN\":\n",
    "                # É uma atribuição: nome = expressão;\n",
    "                self.assign_stmt()\n",
    "            elif self.look_type(1) == \"LPAREN\":\n",
    "                # É uma chamada de função: nome(argumentos);\n",
    "                self.call_stmt()\n",
    "            else:\n",
    "                tok = self.peek()\n",
    "                raise ParseError(f\"Comando iniciado por IDENT inválido em {tok.line}:{tok.col}\")\n",
    "        elif t == \"LBRACE\":\n",
    "            # É um bloco aninhado\n",
    "            self.block()\n",
    "        else:\n",
    "            tok = self.peek()\n",
    "            raise ParseError(f\"Declaração/Comando inválido em {tok.line}:{tok.col} (token={tok.type})\")\n",
    "\n",
    "    def func_decl(self):\n",
    "        \"\"\"Declaração de função sem parâmetros: IDENT '(' ')' block\"\"\"\n",
    "        self.expect(\"IDENT\", \"Esperado nome da função\")\n",
    "        self.expect(\"LPAREN\", \"Esperado '(' após nome da função\")\n",
    "        self.expect(\"RPAREN\", \"Esperado ')' após parâmetros da função\")\n",
    "        self.block()  # Corpo da função\n",
    "\n",
    "    def call_stmt(self):\n",
    "        \"\"\"Chamada de função: IDENT '(' [argumentos] ')' ';'\"\"\"\n",
    "        self.expect(\"IDENT\", \"Esperado nome da função na chamada\")\n",
    "        self.expect(\"LPAREN\", \"Esperado '(' na chamada de função\")\n",
    "        \n",
    "        # Argumentos são opcionais\n",
    "        if self.peek().type != \"RPAREN\":\n",
    "            self.expr()  # Primeiro argumento\n",
    "            while self.match(\"COMMA\"):  # Argumentos adicionais separados por vírgula\n",
    "                self.expr()\n",
    "                \n",
    "        self.expect(\"RPAREN\", \"Esperado ')' ao final da chamada\")\n",
    "        self.expect(\"SEMI\", \"Esperado ';' ao final da chamada de função\")\n",
    "\n",
    "    def if_stmt(self):  \n",
    "        \"\"\"Comando condicional: if '(' expressão ')' comando [else comando]\"\"\"\n",
    "        self.expect(\"IF\", \"Esperado 'if'\")\n",
    "        self.expect(\"LPAREN\", \"Esperado '(' após 'if'\")\n",
    "        self.expr()  # Condição\n",
    "        self.expect(\"RPAREN\", \"Esperado ')' após condição\")\n",
    "        self.stmt()  # Comando do 'then'\n",
    "        \n",
    "        # Else é opcional\n",
    "        if self.match(\"ELSE\"):\n",
    "            self.stmt()  # Comando do 'else'\n",
    "\n",
    "    def while_stmt(self):  \n",
    "        \"\"\"Loop: while '(' expressão ')' comando\"\"\"\n",
    "        self.expect(\"WHILE\", \"Esperado 'while'\")\n",
    "        self.expect(\"LPAREN\", \"Esperado '(' após 'while'\")\n",
    "        self.expr()  # Condição do loop\n",
    "        self.expect(\"RPAREN\", \"Esperado ')' após condição\")\n",
    "        self.stmt()  # Corpo do loop\n",
    "\n",
    "    def assign_stmt(self):  \n",
    "        \"\"\"Atribuição: identificador '=' expressão ';'\"\"\"\n",
    "        self.expect(\"IDENT\", \"Esperado identificador no início da atribuição\")\n",
    "        self.expect(\"ASSIGN\", \"Esperado '=' em atribuição\")\n",
    "        self.expr()  # Valor a ser atribuído\n",
    "        self.expect(\"SEMI\", \"Esperado ';' após atribuição\")\n",
    "\n",
    "    # Métodos para analisar expressões seguindo precedência de operadores\n",
    "    # Cada nível de precedência tem seu próprio método\n",
    "\n",
    "    def expr(self):\n",
    "        \"\"\"Expressão no nível mais alto (menor precedência)\"\"\"\n",
    "        self.or_()\n",
    "        \n",
    "    def or_(self):\n",
    "        \"\"\"Operador lógico OR (||) - associativo à esquerda\"\"\"\n",
    "        self.and_()\n",
    "        while self.match(\"OROR\"):\n",
    "            self.and_()\n",
    "            \n",
    "    def and_(self):\n",
    "        \"\"\"Operador lógico AND (&&) - associativo à esquerda\"\"\"\n",
    "        self.equality()\n",
    "        while self.match(\"ANDAND\"):\n",
    "            self.equality()\n",
    "            \n",
    "    def equality(self):\n",
    "        \"\"\"Operadores de igualdade (== !=) - associativo à esquerda\"\"\"\n",
    "        self.rel()\n",
    "        while self.match(\"EQEQ\", \"NEQ\"):\n",
    "            self.rel()\n",
    "            \n",
    "    def rel(self):\n",
    "        \"\"\"Operadores relacionais (< > <= >=) - associativo à esquerda\"\"\"\n",
    "        self.add()\n",
    "        while self.match(\"LT\", \"GT\", \"LE\", \"GE\"):\n",
    "            self.add()\n",
    "            \n",
    "    def add(self):\n",
    "        \"\"\"Operadores aditivos (+ -) - associativo à esquerda\"\"\"\n",
    "        self.mul()\n",
    "        while self.match(\"PLUS\", \"MINUS\"):\n",
    "            self.mul()\n",
    "            \n",
    "    def mul(self):\n",
    "        \"\"\"Operadores multiplicativos (* / %) - associativo à esquerda\"\"\"\n",
    "        self.unary()\n",
    "        while self.match(\"STAR\", \"SLASH\", \"PERCENT\"):\n",
    "            self.unary()\n",
    "            \n",
    "    def unary(self):\n",
    "        \"\"\"Operadores unários (! -) - associativo à direita\"\"\"\n",
    "        if self.match(\"BANG\", \"MINUS\"):\n",
    "            self.unary()  # Recursão à direita para associatividade correta\n",
    "        else:\n",
    "            self.primary()\n",
    "            \n",
    "    def primary(self):\n",
    "        \"\"\"Expressões primárias (números, booleanos, identificadores, parênteses)\"\"\"\n",
    "        if self.match(\"NUMBER\", \"TRUE\", \"FALSE\", \"IDENT\"):\n",
    "            return\n",
    "        if self.match(\"LPAREN\"):\n",
    "            self.expr()  # Expressão entre parênteses\n",
    "            self.expect(\"RPAREN\", \"Esperado ')' para fechar expressão\")\n",
    "            return\n",
    "        t = self.peek()\n",
    "        raise ParseError(f\"Expressão inválida em {t.line}:{t.col} (token={t.type})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616d80e6",
   "metadata": {},
   "source": [
    "**DEMO: código-fonte de teste**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167826ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código fonte de exemplo para testar nosso analisador léxico e sintático\n",
    "# Contém diversos elementos da linguagem: declarações, atribuições, \n",
    "# condicionais, loops, funções e expressões aritméticas\n",
    "DEMO_SOURCE = r\"\"\"\n",
    "main() {\n",
    "  int x = 10;\n",
    "  bool ok = true;\n",
    "  int y;\n",
    "  y = x + 20 * (3 - 1);\n",
    "\n",
    "  if (y >= 50 && ok) {\n",
    "    print(y);\n",
    "  } else {\n",
    "    y = y - 1;\n",
    "  }\n",
    "\n",
    "soma() {\n",
    "  int a = 1;\n",
    "  int b = 2;\n",
    "  int c = a + b;\n",
    "}\n",
    "  while (y > 0) {\n",
    "    y = y - 7;\n",
    "  }\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668822f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_lexeme_table(tokens: List[Token]):\n",
    "    \"\"\"Exibe uma tabela formatada com todos os tokens encontrados\"\"\"\n",
    "    print(\"=\" * 60)  \n",
    "    print(\"TABELA DE LEXEMAS E TOKENS\")  \n",
    "    print(\"=\" * 60)  \n",
    "    print(f\"{'Linha':<5} {'Col':<3} {'Token':<12} Lexema\")  \n",
    "    print(\"-\" * 60)  \n",
    "    \n",
    "    # Percorre todos os tokens e os exibe em formato tabular\n",
    "    for i, t in enumerate(tokens):\n",
    "        print(f\"{t.line:<5} {t.col:<3} {t.type:<12} {t.lexeme}\")\n",
    "        \n",
    "        # Para listas muito grandes, mostra progresso a cada 20 tokens\n",
    "        if (i + 1) % 20 == 0 and i + 1 < len(tokens):\n",
    "            print(f\"... ({i + 1}/{len(tokens)} tokens exibidos até agora)\")\n",
    "    \n",
    "    print(\"-\" * 60)  \n",
    "    print(f\"Total de tokens: {len(tokens)}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Função principal que demonstra o funcionamento do analisador\"\"\"\n",
    "    print(\"=\" * 60)  \n",
    "    print(\"ANALISADOR LÉXICO E PARSER - MINI LANGUAGE\")  \n",
    "    print(\"=\" * 60)  \n",
    "\n",
    "    # Fase 1: Análise Léxica\n",
    "    lexer = Lexer(DEMO_SOURCE)  # Cria o analisador léxico\n",
    "    tokens = lexer.tokenize()   # Extrai todos os tokens do código fonte\n",
    "\n",
    "    # Verifica se o lexer funcionou corretamente\n",
    "    assert tokens and tokens[-1].type == \"EOF\", \"O lexer deve terminar com EOF\"\n",
    "\n",
    "    # Exibe a tabela de tokens na tela\n",
    "    print_lexeme_table(tokens)\n",
    "\n",
    "    # Cria e exibe a cadeia de tokens (útil para debug)\n",
    "    chain_with_eof = \" \".join(t.type for t in tokens)\n",
    "    print(\"\\nCADEIA DE TOKENS (inclui EOF):\")\n",
    "    print(chain_with_eof)\n",
    "\n",
    "    # Salva os resultados em arquivos para análise posterior\n",
    "    with open(\"tabela_tokens.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"=\" * 60 + \"\\n\")\n",
    "        f.write(\"TABELA DE LEXEMAS E TOKENS\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\")\n",
    "        f.write(f\"{'Linha':<5} {'Col':<3} {'Token':<12} Lexema\\n\")\n",
    "        f.write(\"-\" * 60 + \"\\n\")\n",
    "        for t in tokens:\n",
    "            f.write(f\"{t.line:<5} {t.col:<3} {t.type:<12} {t.lexeme}\\n\")\n",
    "        f.write(\"-\" * 60 + \"\\n\")\n",
    "        f.write(f\"Total de tokens: {len(tokens)}\\n\")\n",
    "        \n",
    "    with open(\"cadeia_tokens.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(chain_with_eof + \"\\n\")\n",
    "    print(\"\\n[OK] Arquivos salvos: tabela_tokens.txt, cadeia_tokens.txt\")\n",
    "\n",
    "    # Fase 2: Análise Sintática\n",
    "    print(\"\\nFASE SINTÁTICA (versão inicial):\")\n",
    "    parser = Parser(tokens)  # Cria o analisador sintático\n",
    "    try:\n",
    "        parser.parse()  # Tenta analisar a sequência de tokens\n",
    "        print(\"✅ Parse concluído sem erros (programa reconhecido).\")  \n",
    "    except ParseError as e:\n",
    "        print(f\"❌ Erro de sintaxe: {e}\")  \n",
    "    except SyntaxError as e:\n",
    "        print(f\"❌ Erro léxico (inesperado): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272afa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa sys para controlar a saída de forma mais precisa\n",
    "import sys\n",
    "\n",
    "def print_lexeme_table(tokens: List[Token]):\n",
    "    \"\"\"Versão alternativa da função que exibe tokens com formatação ligeiramente diferente\"\"\"\n",
    "    sys.stdout.write(\"=\" * 72 + \"\\n\")\n",
    "    sys.stdout.write(\"TABELA DE LEXEMAS E TOKENS\\n\")\n",
    "    sys.stdout.write(\"=\" * 72 + \"\\n\")\n",
    "    sys.stdout.write(f\"{'Linha':<6} {'Col':<4} {'Token':<12} Lexema\\n\")\n",
    "    sys.stdout.write(\"-\" * 72 + \"\\n\")\n",
    "    \n",
    "    # Exibe cada token usando sys.stdout.write para controle direto da saída\n",
    "    for t in tokens:  \n",
    "        sys.stdout.write(f\"{t.line:<6} {t.col:<4} {t.type:<12} {t.lexeme}\\n\")\n",
    "    sys.stdout.write(\"-\" * 72 + \"\\n\")\n",
    "    sys.stdout.flush()  # Força a saída imediata\n",
    "\n",
    "def main():\n",
    "    \"\"\"Versão simplificada da função principal para demonstração\"\"\"\n",
    "    print(\"=\" * 72)\n",
    "    print(\"ANALISADOR LÉXICO E PARSER - MINI LANGUAGE\")\n",
    "    print(\"=\" * 72)\n",
    "\n",
    "    # Executa apenas a análise léxica\n",
    "    lexer = Lexer(DEMO_SOURCE)\n",
    "    tokens = lexer.tokenize()\n",
    "\n",
    "    # Verifica integridade dos tokens\n",
    "    assert tokens and tokens[-1].type == \"EOF\", \"O lexer deve terminar com EOF\"\n",
    "\n",
    "    # Exibe resultados\n",
    "    print_lexeme_table(tokens)\n",
    "\n",
    "    print(\"\\nCADEIA DE TOKENS (inclui EOF):\")\n",
    "    print(\" \".join(t.type for t in tokens))\n",
    "\n",
    "    # Executa análise sintática\n",
    "    print(\"\\nFASE SINTÁTICA (versão inicial):\")\n",
    "    parser = Parser(tokens)\n",
    "    try:\n",
    "        parser.parse()\n",
    "        print(\"✅ Parse concluído sem erros (programa reconhecido).\")\n",
    "    except ParseError as e:\n",
    "        print(f\"❌ Erro de sintaxe: {e}\")\n",
    "    except SyntaxError as e:\n",
    "        print(f\"❌ Erro léxico (inesperado): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533eed80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "ANALISADOR LÉXICO E PARSER - MINI LANGUAGE\n",
      "========================================================================\n",
      "========================================================================\n",
      "TABELA DE LEXEMAS E TOKENS\n",
      "========================================================================\n",
      "Linha  Col  Token        Lexema\n",
      "------------------------------------------------------------------------\n",
      "3      1    IDENT        main\n",
      "3      5    LPAREN       (\n",
      "3      6    RPAREN       )\n",
      "3      8    LBRACE       {\n",
      "4      3    INT          int\n",
      "4      7    IDENT        x\n",
      "4      9    ASSIGN       =\n",
      "4      11   NUMBER       10\n",
      "4      13   SEMI         ;\n",
      "5      3    BOOL         bool\n",
      "5      8    IDENT        ok\n",
      "5      11   ASSIGN       =\n",
      "5      13   TRUE         true\n",
      "5      17   SEMI         ;\n",
      "6      3    INT          int\n",
      "6      7    IDENT        y\n",
      "6      8    SEMI         ;\n",
      "7      3    IDENT        y\n",
      "7      5    ASSIGN       =\n",
      "7      7    IDENT        x\n",
      "7      9    PLUS         +\n",
      "7      11   NUMBER       20\n",
      "7      14   STAR         *\n",
      "7      16   LPAREN       (\n",
      "7      17   NUMBER       3\n",
      "7      19   MINUS        -\n",
      "7      21   NUMBER       1\n",
      "7      22   RPAREN       )\n",
      "7      23   SEMI         ;\n",
      "10     1    IDENT        soma\n",
      "10     5    LPAREN       (\n",
      "10     6    RPAREN       )\n",
      "10     8    LBRACE       {\n",
      "11     3    INT          int\n",
      "11     7    IDENT        a\n",
      "11     9    ASSIGN       =\n",
      "11     11   NUMBER       1\n",
      "11     12   SEMI         ;\n",
      "12     3    INT          int\n",
      "12     7    IDENT        b\n",
      "12     9    ASSIGN       =\n",
      "12     11   NUMBER       2\n",
      "12     12   SEMI         ;\n",
      "13     3    INT          int\n",
      "13     7    IDENT        c\n",
      "13     9    ASSIGN       =\n",
      "13     11   IDENT        a\n",
      "13     13   PLUS         +\n",
      "13     15   IDENT        b\n",
      "13     16   SEMI         ;\n",
      "14     1    RBRACE       }\n",
      "15     3    WHILE        while\n",
      "15     9    LPAREN       (\n",
      "15     10   IDENT        y\n",
      "15     12   GT           >\n",
      "15     14   NUMBER       0\n",
      "15     15   RPAREN       )\n",
      "15     17   LBRACE       {\n",
      "16     5    IDENT        y\n",
      "16     7    ASSIGN       =\n",
      "16     9    IDENT        y\n",
      "16     11   MINUS        -\n",
      "16     13   NUMBER       7\n",
      "16     14   SEMI         ;\n",
      "17     3    RBRACE       }\n",
      "18     1    RBRACE       }\n",
      "20     1    EOF          \n",
      "------------------------------------------------------------------------\n",
      "\n",
      "CADEIA DE TOKENS (inclui EOF):\n",
      "IDENT LPAREN RPAREN LBRACE INT IDENT ASSIGN NUMBER SEMI BOOL IDENT ASSIGN TRUE SEMI INT IDENT SEMI IDENT ASSIGN IDENT PLUS NUMBER STAR LPAREN NUMBER MINUS NUMBER RPAREN SEMI IDENT LPAREN RPAREN LBRACE INT IDENT ASSIGN NUMBER SEMI INT IDENT ASSIGN NUMBER SEMI INT IDENT ASSIGN IDENT PLUS IDENT SEMI RBRACE WHILE LPAREN IDENT GT NUMBER RPAREN LBRACE IDENT ASSIGN IDENT MINUS NUMBER SEMI RBRACE RBRACE EOF\n",
      "\n",
      "FASE SINTÁTICA (versão inicial):\n",
      "✅ Parse concluído sem erros (programa reconhecido).\n"
     ]
    }
   ],
   "source": [
    "# Ponto de entrada do programa - executa a função main quando o script é rodado diretamente\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d469d965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÁRVORE DE SINTAXE (ASCII):\n",
      "└── Program\n",
      "    └── Func(main)\n",
      "        └── Block\n",
      "            ├── DeclList\n",
      "            │   ├── Decl\n",
      "            │   │   ├── int\n",
      "            │   │   ├── Id(x)\n",
      "            │   │   └── Init\n",
      "            │   │       └── Num(10)\n",
      "            │   ├── Decl\n",
      "            │   │   ├── bool\n",
      "            │   │   ├── Id(ok)\n",
      "            │   │   └── Init\n",
      "            │   │       └── Bool(true)\n",
      "            │   └── Decl\n",
      "            │       ├── int\n",
      "            │       └── Id(y)\n",
      "            └── StmtList\n",
      "                ├── Assign\n",
      "                │   ├── Id(y)\n",
      "                │   └── +\n",
      "                │       ├── Id(x)\n",
      "                │       └── *\n",
      "                │           ├── Num(20)\n",
      "                │           └── ( )\n",
      "                │               └── -\n",
      "                │                   ├── Num(3)\n",
      "                │                   └── Num(1)\n",
      "                ├── IfElse\n",
      "                │   ├── &&\n",
      "                │   │   ├── >=\n",
      "                │   │   │   ├── Id(y)\n",
      "                │   │   │   └── Num(50)\n",
      "                │   │   └── Id(ok)\n",
      "                │   ├── Block\n",
      "                │   │   ├── DeclList\n",
      "                │   │   └── StmtList\n",
      "                │   │       └── Call(print)\n",
      "                │   │           └── Args\n",
      "                │   │               └── Id(y)\n",
      "                │   └── Block\n",
      "                │       ├── DeclList\n",
      "                │       └── StmtList\n",
      "                │           └── Assign\n",
      "                │               ├── Id(y)\n",
      "                │               └── -\n",
      "                │                   ├── Id(y)\n",
      "                │                   └── Num(1)\n",
      "                ├── Func(soma)\n",
      "                │   └── Block\n",
      "                │       ├── DeclList\n",
      "                │       │   ├── Decl\n",
      "                │       │   │   ├── int\n",
      "                │       │   │   ├── Id(a)\n",
      "                │       │   │   └── Init\n",
      "                │       │   │       └── Num(1)\n",
      "                │       │   ├── Decl\n",
      "                │       │   │   ├── int\n",
      "                │       │   │   ├── Id(b)\n",
      "                │       │   │   └── Init\n",
      "                │       │   │       └── Num(2)\n",
      "                │       │   └── Decl\n",
      "                │       │       ├── int\n",
      "                │       │       ├── Id(c)\n",
      "                │       │       └── Init\n",
      "                │       │           └── +\n",
      "                │       │               ├── Id(a)\n",
      "                │       │               └── Id(b)\n",
      "                │       └── StmtList\n",
      "                └── While\n",
      "                    ├── >\n",
      "                    │   ├── Id(y)\n",
      "                    │   └── Num(0)\n",
      "                    └── Block\n",
      "                        ├── DeclList\n",
      "                        └── StmtList\n",
      "                            └── Assign\n",
      "                                ├── Id(y)\n",
      "                                └── -\n",
      "                                    ├── Id(y)\n",
      "                                    └── Num(7)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ASTNode:\n",
    "    \"\"\"\n",
    "    Representa um nó na Árvore de Sintaxe Abstrata (AST).\n",
    "    Cada nó tem um rótulo que descreve o que representa\n",
    "    e pode ter filhos que são outros nós da árvore.\n",
    "    \"\"\"\n",
    "    label: str                              # Nome/tipo do nó (ex: \"If\", \"Assign\", \"Num(5)\")\n",
    "    children: List[\"ASTNode\"] = field(default_factory=list)  # Lista de nós filhos\n",
    "    \n",
    "    def add(self, *nodes):\n",
    "        \"\"\"Adiciona um ou mais nós como filhos deste nó\"\"\"\n",
    "        for n in nodes:\n",
    "            if n is not None:  # Só adiciona nós válidos\n",
    "                self.children.append(n)\n",
    "        return self  # Retorna self para permitir encadeamento\n",
    "\n",
    "def print_ast_ascii(node: ASTNode, prefix: str = \"\", is_last: bool = True):\n",
    "    \"\"\"\n",
    "    Exibe a árvore AST em formato ASCII art, facilitando a visualização\n",
    "    da estrutura hierárquica do programa analisado\n",
    "    \"\"\"\n",
    "    # Escolhe o símbolo correto baseado se é o último filho ou não\n",
    "    elbow = \"└── \" if is_last else \"├── \"\n",
    "    print(prefix + elbow + node.label)\n",
    "    \n",
    "    # Calcula o prefixo para os filhos baseado na posição atual\n",
    "    child_prefix = prefix + (\"    \" if is_last else \"│   \")\n",
    "    \n",
    "    # Recursivamente imprime todos os filhos\n",
    "    for i, ch in enumerate(node.children):\n",
    "        print_ast_ascii(ch, child_prefix, i == len(node.children) - 1)\n",
    "\n",
    "class ParserAST:\n",
    "    \"\"\"\n",
    "    Versão do parser que constrói uma Árvore de Sintaxe Abstrata (AST)\n",
    "    em vez de apenas validar a sintaxe. Cada método retorna um nó AST\n",
    "    que representa a estrutura sintática encontrada.\n",
    "    \n",
    "    A AST é útil para:\n",
    "    - Visualizar a estrutura do programa\n",
    "    - Implementar interpretadores\n",
    "    - Fazer análise semântica\n",
    "    - Gerar código\n",
    "    \"\"\"\n",
    "    def __init__(self, tokens: List[\"Token\"]):\n",
    "        self.tokens = tokens\n",
    "        self.pos = 0\n",
    "\n",
    "    def peek(self) -> \"Token\":\n",
    "        \"\"\"Olha o token atual sem avançar\"\"\"\n",
    "        return self.tokens[self.pos]\n",
    "\n",
    "    def advance(self) -> \"Token\":\n",
    "        \"\"\"Consome e retorna o token atual\"\"\"\n",
    "        tok = self.tokens[self.pos]\n",
    "        self.pos += 1\n",
    "        return tok\n",
    "\n",
    "    def match(self, *types) -> Optional[\"Token\"]:\n",
    "        \"\"\"Verifica se o token atual é de um dos tipos dados e o consome se for\"\"\"\n",
    "        if self.peek().type in types:\n",
    "            return self.advance()\n",
    "        return None\n",
    "\n",
    "    def expect(self, ttype: str, msg: str) -> \"Token\":\n",
    "        \"\"\"Exige que o token atual seja de um tipo específico\"\"\"\n",
    "        if self.peek().type == ttype:\n",
    "            return self.advance()\n",
    "        t = self.peek()\n",
    "        raise ParseError(f\"{msg} (encontrado {t.type} '{t.lexeme}' em {t.line}:{t.col})\")\n",
    "\n",
    "    def look_type(self, k: int) -> str:\n",
    "        \"\"\"Olha o tipo do token k posições à frente\"\"\"\n",
    "        idx = self.pos + k\n",
    "        return self.tokens[idx].type if idx < len(self.tokens) else \"EOF\"\n",
    "\n",
    "    def parse(self) -> ASTNode:\n",
    "        \"\"\"Método principal que analisa o programa e retorna a raiz da AST\"\"\"\n",
    "        # Verifica se começamos com 'main'\n",
    "        if not (self.peek().type == \"IDENT\" and self.peek().lexeme == \"main\"):\n",
    "            t = self.peek()\n",
    "            raise ParseError(f\"Esperado identificador 'main' (encontrado {t.type} '{t.lexeme}' em {t.line}:{t.col})\")\n",
    "        \n",
    "        self.advance()  # Consome 'main'\n",
    "        self.expect(\"LPAREN\", \"Esperado '(' após 'main'\")\n",
    "        self.expect(\"RPAREN\", \"Esperado ')' após '('\")\n",
    "        \n",
    "        # Analisa o bloco principal\n",
    "        main_block = self.block()\n",
    "        self.expect(\"EOF\", \"Esperado fim do arquivo\")\n",
    "\n",
    "        # Cria e retorna a raiz da AST\n",
    "        root = ASTNode(\"Program\").add(ASTNode(\"Func(main)\").add(main_block))\n",
    "        return root\n",
    "\n",
    "    def block(self) -> ASTNode:\n",
    "        \"\"\"Analisa um bloco e retorna nó AST correspondente\"\"\"\n",
    "        self.expect(\"LBRACE\", \"Esperado '{'\")\n",
    "        \n",
    "        # Coleta todas as declarações\n",
    "        decls = ASTNode(\"DeclList\")\n",
    "        while self.peek().type in (\"INT\", \"BOOL\"):\n",
    "            decls.add(self.decl())\n",
    "            \n",
    "        # Coleta todos os comandos\n",
    "        stmts = ASTNode(\"StmtList\")\n",
    "        while self.peek().type in (\"IF\", \"WHILE\", \"IDENT\", \"LBRACE\"):\n",
    "            stmts.add(self.stmt())\n",
    "            \n",
    "        self.expect(\"RBRACE\", \"Esperado '}'\")\n",
    "        return ASTNode(\"Block\").add(decls, stmts)\n",
    "\n",
    "    def decl(self) -> ASTNode:\n",
    "        \"\"\"Analisa uma declaração de variável\"\"\"\n",
    "        typ = self.advance()  # Consome o tipo (INT ou BOOL)\n",
    "        ident = self.expect(\"IDENT\", \"Esperado identificador\")\n",
    "        \n",
    "        # Cria nó da declaração\n",
    "        node = ASTNode(\"Decl\").add(ASTNode(typ.type.lower()), ASTNode(f\"Id({ident.lexeme})\"))\n",
    "        \n",
    "        # Verifica se há inicialização\n",
    "        if self.match(\"ASSIGN\"):\n",
    "            node.add(ASTNode(\"Init\").add(self.expr()))\n",
    "            \n",
    "        self.expect(\"SEMI\", \"Esperado ';'\")\n",
    "        return node\n",
    "\n",
    "    def stmt(self) -> ASTNode:\n",
    "        \"\"\"Analisa um comando e retorna nó AST apropriado\"\"\"\n",
    "        t = self.peek().type\n",
    "        \n",
    "        if t == \"IF\":\n",
    "            return self.if_stmt()\n",
    "        if t == \"WHILE\":\n",
    "            return self.while_stmt()\n",
    "        if t == \"LBRACE\":\n",
    "            return self.block()\n",
    "        if t == \"IDENT\":\n",
    "            # Decide que tipo de comando é baseado no lookahead\n",
    "            if (self.look_type(1) == \"LPAREN\"\n",
    "                and self.look_type(2) == \"RPAREN\"\n",
    "                and self.look_type(3) == \"LBRACE\"):\n",
    "                return self.func_decl()  # Declaração de função\n",
    "            if self.look_type(1) == \"ASSIGN\":\n",
    "                return self.assign_stmt()  # Atribuição\n",
    "            if self.look_type(1) == \"LPAREN\":\n",
    "                return self.call_stmt()  # Chamada de função\n",
    "                \n",
    "        tok = self.peek()\n",
    "        raise ParseError(f\"Comando inválido em {tok.line}:{tok.col} (token={tok.type})\")\n",
    "\n",
    "    def func_decl(self) -> ASTNode:\n",
    "        \"\"\"Analisa declaração de função\"\"\"\n",
    "        name_tok = self.expect(\"IDENT\", \"Esperado nome da função\")\n",
    "        self.expect(\"LPAREN\", \"Esperado '(' após nome da função\")\n",
    "        self.expect(\"RPAREN\", \"Esperado ')' após parâmetros da função\")\n",
    "        body = self.block()\n",
    "        return ASTNode(f\"Func({name_tok.lexeme})\").add(body)\n",
    "\n",
    "    def call_stmt(self) -> ASTNode:\n",
    "        \"\"\"Analisa chamada de função\"\"\"\n",
    "        name_tok = self.expect(\"IDENT\", \"Esperado nome da função na chamada\")\n",
    "        self.expect(\"LPAREN\", \"Esperado '(' na chamada de função\")\n",
    "        \n",
    "        # Coleta argumentos\n",
    "        args = ASTNode(\"Args\")\n",
    "        if self.peek().type != \"RPAREN\":\n",
    "            args.add(self.expr())\n",
    "            while self.match(\"COMMA\"):\n",
    "                args.add(self.expr())\n",
    "                \n",
    "        self.expect(\"RPAREN\", \"Esperado ')' ao final da chamada\")\n",
    "        self.expect(\"SEMI\", \"Esperado ';' ao final da chamada de função\")\n",
    "        return ASTNode(f\"Call({name_tok.lexeme})\").add(args)\n",
    "\n",
    "    def if_stmt(self) -> ASTNode:\n",
    "        \"\"\"Analisa comando if/else\"\"\"\n",
    "        self.expect(\"IF\", \"Esperado 'if'\")\n",
    "        self.expect(\"LPAREN\", \"Esperado '('\")\n",
    "        cond = self.expr()  # Condição\n",
    "        self.expect(\"RPAREN\", \"Esperado ')'\")\n",
    "        thenp = self.stmt()  # Comando do then\n",
    "        \n",
    "        # Verifica se há else\n",
    "        if self.match(\"ELSE\"):\n",
    "            elsep = self.stmt()\n",
    "            return ASTNode(\"IfElse\").add(cond, thenp, elsep)\n",
    "        return ASTNode(\"If\").add(cond, thenp)\n",
    "\n",
    "    def while_stmt(self) -> ASTNode:\n",
    "        \"\"\"Analisa comando while\"\"\"\n",
    "        self.expect(\"WHILE\", \"Esperado 'while'\")\n",
    "        self.expect(\"LPAREN\", \"Esperado '('\")\n",
    "        cond = self.expr()  # Condição\n",
    "        self.expect(\"RPAREN\", \"Esperado ')'\")\n",
    "        body = self.stmt()  # Corpo do loop\n",
    "        return ASTNode(\"While\").add(cond, body)\n",
    "\n",
    "    def assign_stmt(self) -> ASTNode:\n",
    "        \"\"\"Analisa comando de atribuição\"\"\"\n",
    "        ident = self.expect(\"IDENT\", \"Esperado identificador\")\n",
    "        self.expect(\"ASSIGN\", \"Esperado '='\")\n",
    "        e = self.expr()  # Expressão do lado direito\n",
    "        self.expect(\"SEMI\", \"Esperado ';' após atribuição\")\n",
    "        return ASTNode(\"Assign\").add(ASTNode(f\"Id({ident.lexeme})\"), e)\n",
    "\n",
    "    def expr(self) -> ASTNode:\n",
    "        \"\"\"Ponto de entrada para análise de expressões\"\"\"\n",
    "        return self.or_()\n",
    "\n",
    "    def or_(self) -> ASTNode:\n",
    "        \"\"\"Operador lógico OR - menor precedência\"\"\"\n",
    "        node = self.and_()\n",
    "        while self.match(\"OROR\"):\n",
    "            node = ASTNode(\"||\").add(node, self.and_())\n",
    "        return node\n",
    "\n",
    "    def and_(self) -> ASTNode:\n",
    "        \"\"\"Operador lógico AND\"\"\"\n",
    "        node = self.equality()\n",
    "        while self.match(\"ANDAND\"):\n",
    "            node = ASTNode(\"&&\").add(node, self.equality())\n",
    "        return node\n",
    "\n",
    "    def equality(self) -> ASTNode:\n",
    "        \"\"\"Operadores de igualdade\"\"\"\n",
    "        node = self.rel()\n",
    "        while True:\n",
    "            if self.match(\"EQEQ\"):\n",
    "                node = ASTNode(\"==\").add(node, self.rel())\n",
    "            elif self.match(\"NEQ\"):\n",
    "                node = ASTNode(\"!=\").add(node, self.rel())\n",
    "            else:\n",
    "                break\n",
    "        return node\n",
    "\n",
    "    def rel(self) -> ASTNode:\n",
    "        \"\"\"Operadores relacionais\"\"\"\n",
    "        node = self.add()\n",
    "        while True:\n",
    "            if self.match(\"LT\"):\n",
    "                node = ASTNode(\"<\").add(node, self.add())\n",
    "            elif self.match(\"GT\"):\n",
    "                node = ASTNode(\">\").add(node, self.add())\n",
    "            elif self.match(\"LE\"):\n",
    "                node = ASTNode(\"<=\").add(node, self.add())\n",
    "            elif self.match(\"GE\"):\n",
    "                node = ASTNode(\">=\").add(node, self.add())\n",
    "            else:\n",
    "                break\n",
    "        return node\n",
    "\n",
    "    def add(self) -> ASTNode:\n",
    "        \"\"\"Operadores aditivos\"\"\"\n",
    "        node = self.mul()\n",
    "        while True:\n",
    "            if self.match(\"PLUS\"):\n",
    "                node = ASTNode(\"+\").add(node, self.mul())\n",
    "            elif self.match(\"MINUS\"):\n",
    "                node = ASTNode(\"-\").add(node, self.mul())\n",
    "            else:\n",
    "                break\n",
    "        return node\n",
    "\n",
    "    def mul(self) -> ASTNode:\n",
    "        \"\"\"Operadores multiplicativos\"\"\"\n",
    "        node = self.unary()\n",
    "        while True:\n",
    "            if self.match(\"STAR\"):\n",
    "                node = ASTNode(\"*\").add(node, self.unary())\n",
    "            elif self.match(\"SLASH\"):\n",
    "                node = ASTNode(\"/\").add(node, self.unary())\n",
    "            elif self.match(\"PERCENT\"):\n",
    "                node = ASTNode(\"%\").add(node, self.unary())\n",
    "            else:\n",
    "                break\n",
    "        return node\n",
    "\n",
    "    def unary(self) -> ASTNode:\n",
    "        \"\"\"Operadores unários - maior precedência\"\"\"\n",
    "        if self.match(\"BANG\"):\n",
    "            return ASTNode(\"!\").add(self.unary())\n",
    "        if self.match(\"MINUS\"):\n",
    "            return ASTNode(\"neg\").add(self.unary())  # Negativo unário\n",
    "        return self.primary()\n",
    "\n",
    "    def primary(self) -> ASTNode:\n",
    "        \"\"\"Expressões primárias - folhas da árvore\"\"\"\n",
    "        if (tok := self.match(\"NUMBER\")):\n",
    "            return ASTNode(f\"Num({tok.lexeme})\")\n",
    "        if (tok := self.match(\"TRUE\")):\n",
    "            return ASTNode(\"Bool(true)\")\n",
    "        if (tok := self.match(\"FALSE\")):\n",
    "            return ASTNode(\"Bool(false)\")\n",
    "        if (tok := self.match(\"IDENT\")):\n",
    "            return ASTNode(f\"Id({tok.lexeme})\")\n",
    "        if self.match(\"LPAREN\"):\n",
    "            e = self.expr()  # Expressão entre parênteses\n",
    "            self.expect(\"RPAREN\", \"Esperado ')'\")\n",
    "            return ASTNode(\"( )\").add(e)  # Nó para indicar parênteses\n",
    "            \n",
    "        t = self.peek()\n",
    "        raise ParseError(f\"Expressão inválida em {t.line}:{t.col} (token={t.type})\")\n",
    "\n",
    "# Demonstração: cria AST do código de exemplo e a exibe\n",
    "tokens_ast = Lexer(DEMO_SOURCE).tokenize()  # Tokeniza o código\n",
    "ast_root = ParserAST(tokens_ast).parse()    # Constrói a AST\n",
    "print(\"ÁRVORE DE SINTAXE (ASCII):\")\n",
    "print_ast_ascii(ast_root)  # Exibe a árvore em formato visual"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
